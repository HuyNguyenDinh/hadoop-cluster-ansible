- name: Download spark tarball
  ansible.builtin.get_url:
    url: "{{ spark_download_url }}"
    dest: "/tmp/spark-{{ spark_version }}.tgz"
    mode: '0644'

- name: Extract spark tarball
  ansible.builtin.unarchive:
    src: "/tmp/spark-{{ spark_version }}.tgz"
    dest: /opt/
    remote_src: yes
    creates: "/opt/spark-{{ spark_version }}"

- name: Create symlink to spark installation directory
  ansible.builtin.file:
    src: "/opt/spark-{{ spark_version }}-bin-hadoop3"
    dest: "{{ spark_install_dir }}"
    state: link
    force: yes

- name: Copy spark environment variables file
  ansible.builtin.copy:
    src: "{{ spark_install_dir }}/conf/spark-env.sh.template"
    remote_src: true
    dest: "{{ spark_install_dir }}/conf/spark-env.sh"
    owner: "{{ hive_user }}"
    group: "{{ hadoop_group }}"
    mode: "0775"

- name: Copy startup Hive script
  ansible.builtin.copy:
    src: "hdfs_id_rsa.pub"
    dest: "/home/{{ hdfs_user }}/.ssh/authorized_keys"
    owner: "{{ hdfs_user }}"
    group: "{{ hadoop_group }}"
    mode: '0600'

- name: Update HADOOP_CONF_DIR in spark-env.sh
  ansible.builtin.lineinfile:
    path: "{{ spark_install_dir }}/conf/spark-env.sh"
    line: "export HADOOP_CONF_DIR={{ hadoop_install_dir }}/etc/hadoop"
    state: present
    mode: '0775'
    owner: "{{ hive_user }}"
    group: "{{ hadoop_group }}"
    create: yes

- name: Update SPARK_DIST_CLASSPATH in spark-env.sh
  ansible.builtin.lineinfile:
    path: "{{ spark_install_dir }}/conf/spark-env.sh"
    line: 'export SPARK_DIST_CLASSPATH={{ hadoop_install_dir }}/bin/hadoop classpath}'
    state: present
    mode: '0775'
    owner: "{{ hive_user }}"
    group: "{{ hadoop_group }}"
    create: yes

- name: Download DB driver
  ansible.builtin.get_url:
    url: "{{ hive_metastore_db_driver_url }}"
    dest: "{{ spark_install_dir }}/jars"
    mode: '0644'

- name: Download MySQL JDBC driver
  ansible.builtin.command:
    cmd: "wget -O /tmp/{{ mysql_connector_filename }} {{ mysql_db_driver_url }}"
  become: yes

- name: Ensure MySQL JDBC destination directory exists
  ansible.builtin.file:
    path: /tmp/mysql-connector-java
    state: directory
    mode: '0755'

- name: Extract MySQL JDBC driver
  ansible.builtin.unarchive:
    src: "/tmp/{{ mysql_connector_filename }}"
    dest: /tmp/mysql-connector-java/
    remote_src: yes

- name: Find MySQL JDBC jar file
  ansible.builtin.find:
    paths: "/tmp/mysql-connector-java/{{ mysql_connector_filename | regex_replace('.tar.gz', '') }}"
    patterns: "mysql-connector-java-*.jar"
  register: jdbc_jar_file

- name: Copy JDBC jar to Spark jars directory
  ansible.builtin.copy:
    src: "{{ jdbc_jar_file.files[0].path }}"
    dest: "{{ spark_install_dir }}/jars/"
    mode: '0644'
    remote_src: yes

- name: Clean up temporary MySQL files
  ansible.builtin.file:
    path: "/tmp/{{ mysql_connector_filename }}"
    state: absent

- name: Clean up temporary MySQL files
  ansible.builtin.file:
    path: /tmp/mysql-connector-java
    state: absent

- name: Download Delta Core driver
  ansible.builtin.get_url:
    url: "{{ delta_core_driver_url }}"
    dest: "{{ spark_install_dir }}/jars"
    mode: '0644'

- name: Download Delta Storage driver
  ansible.builtin.get_url:
    url: "{{ delta_storage_driver_url }}"
    dest: "{{ spark_install_dir }}/jars"
    mode: '0644'

- name: Configure hive-site.xml
  ansible.builtin.template:
    src: hive-site.xml.j2
    dest: "{{ spark_install_dir }}/conf/hive-site.xml"
    owner: "{{ hive_user }}"
    group: "{{ hadoop_group }}"
    mode: '0644'

- name: Create custom start-thriftserver script
  copy:
    dest: "{{ spark_install_dir }}/sbin/start-hive.sh"
    mode: '0755'
    owner: "{{ hive_user }}"
    group: "{{ hadoop_group }}"
    content: |
      #!/bin/bash
      {{ spark_install_dir }}/sbin/start-thriftserver.sh --master yarn --deploy-mode client \
        --driver-memory 4g \
        --executor-memory 6g \
        --executor-cores 4 \
        --num-executors 2 \
        --conf spark.sql.catalogImplementation=hive \
        --conf spark.hadoop.datanucleus.schema.autoCreateTables=true \
        --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
        --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"

- name: Change ownership of spark installation directory
  ansible.builtin.file:
    path: "{{ spark_install_dir }}"
    owner: "{{ hive_user }}"
    group: "{{ hadoop_group }}"
    mode: "0775"
    recurse: yes
    state: directory